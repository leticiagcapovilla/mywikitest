---
title: Alarm system
description: 
published: 1
date: 2024-05-17T21:25:29.366Z
tags: 
editor: markdown
dateCreated: 2024-05-17T21:15:24.651Z
---

# CON: Control-beast

<br>

## Introduction 

In an environment composed of hundreds of thousands of EPICS variables, such as what will be implemented in Sirius, the need for a system capable of monitoring which variables are in the wrong state becomes essential. Therefore, the BEAST (Best Ever Alarm System Toolkit) Alarm Monitor, developed by the [Oak Ridge National Laboratory](https://www.ornl.gov), represents a solution capable of managing and controlling the alarms generated by EPICS servers available on the network. Such a system is implemented in Java and is based on the Eclipse graphical development environment. The architecture of the system is shown in the figure to the right.

<br>

|![](/img/groups/con/control_beast/Beast-arquitetura.png)|
|-|
|**Figure 1**: Alarm monitoring system overview..|

It is possible to distinguish some components in this figure:

* **Alarm server**: the server is responsible for fundamental tasks in the alarm monitoring. It's up to it to read the configuration of the alarms stored in the database ''Alarm Cfg & State RDB'', to connect to the respective variables, to watch their state changes and to activate or deactivate alarms when they are detected or acknowledged by the operators, respectively. This module allows that a variable number of clients connect to it. any variable can adopt two different settings, called ''latch'' and ''annunciate''. For the first one, the server keeps the highest severity alarm until the moment that it is acknowledged, even that the variable is no longer in a alarm state. In order to prevent a big volume of generated alarms, it is possible to enable the options ''delay'', which triggers an alarm only if the wrong state of the variable is kept by that amount of time, and ''count'', which triggers the alarm only if the alarm were detected more than that number of times. 

* **Alarm Cfg & State RDB**: relational database, such as a [PostgreSQL](https://www.postgresql.org) server for instance, into which the alarm configuration and states are stored.
* **Java Message Service - JMS**: used for the communication of the different modules which compose the system. In this project, a solution made by the Apache Software Foundation, called [Apache ActiveMQ](http://activemq.apache.org), was used.

<br>

## Installation 

This subsection is dedicated to the  steps needed for the installation and configuration of the BEAST alarm system.

<br>

### Virtualization 

As in the case of the [CON:Control-archiver|variables archiver](link), we chose to run all components described above in a completely virtualized environment through the use of Docker containers. Despise of presenting a smaller configuration complexity than the one obtained for the archiver, the choice of virtualizing with containers grants an high level of flexibility and simplicity in the system deployment as is explained in the following sections.

<br>

#### Structure 

In total, three containers were developed and compose the system. The first one encapsulates the alarm server, while the other two host the ''postgresql'' database and the inter-process message enchanging module, which is the ''Java Message Service''. The comunication between these containers occurs thanks to an internal network, provided by the Docker itself and that will be discussed afterwards.

<br>

#### Docker Images

The scripts which build the Docker images on which the containers are based can be obtained from three distinct ''git'' repositories, [docker-alarm-activemq](https://github.com/lnls-sirius/docker-alarm-activemq), [docker-alarm-postgres-db](https://github.com/lnls-sirius/docker-alarm-postgres-db) and [docker-alarm-server](https://github.com/lnls-sirius/docker-alarm-server). Some important aspects can be highlighted about these images:

* All definitions used by the alarm server are stored in the file `configuration/LNLS-CON.ini` of [docker-alarm-server](https://github.com/lnls-sirius/docker-alarm-server). Between the main attributes defined in this file, we point out the parameters used by the EPICS libraries to communicate with the IOCs, the database connection properties, such as user, database name and password, and the IP address of Java Message Service. An eventual modification of this file requires a complete image rebuilding, which can be done by executing the following command in the same directory where [docker-alarm-server](https://github.com/lnls-sirius/docker-alarm-server) was cloned.

```
$ build-docker-alarm-server.sh
```

The image is available for downloading from [this link](https://hub.docker.com/r/lnlscon/alarm-server).

* The [image](https://hub.docker.com/r/lnlscon/alarm-postgres-db/)  which encapsulates the database is based on [postgres:latest](https://hub.docker.com/_/postgres/). Its single particularity resides in the fact that it executes the ''script'' `ALARM_POSTGRES.sql`, responsible for populating the database with all required tables, as soon as it is initialized.

* The [image](https://hub.docker.com/r/lnlscon/alarm-server-activemq/) defined by [docker-alarm-activemq](https://github.com/lnls-sirius/docker-alarm-activemq) just starts JMS service, without any peculiarity.

* In all projects discussed above, building and running scripts were implemented. Such files should only be used in development environment, since other more efficient deployment tools can be used. These tools are commented in the next section.

<br>

#### Deployment 

Such as for the [CON:Control-archiver|variables archiver](link), scripts for three container deployment tools are made available: [Docker Compose](https://docs.docker.com/compose/), [Kubernetes](https://kubernetes.io/) and [Docker Swarm](https://docs.docker.com/engine/swarm/).

<br>

##### Docker Compose 

The configuration file `docker-compose.yml` for this tool can be found in the repository [docker-alarm-composed](https://github.com/lnls-sirius/docker-alarm-composed). Each container is specified in the section `services`:

* Container `alarm-server` uses a file (`lnls-beast-server.env`) containing some environment variables which the ''Channel Access'' libraries will use to connect to the variable IOCs.
* Besides receiving definitions of environment variables, the container `alarm-server-postgres-db` should use a persistent data volume, in order to prevent losing all database when the respective container is removed.
* All containers have their IP addresses defined for a internal static address beloging to the network created by the Docker.
* All exposed ports of each container must be set according to what is defined in `configuration/LNLS-CON.ini`, described in section [CON:Control-beast#Imagens|Imagens](link).

To enable the `systemd` service which will launch all the containers as specified in `docker-compose.yml`, execute in the same dirctory where the repository was cloned

```
make install
```

and to disable 

```
make uninstall
```

<br>

###### Debugging 

The running state of the container containing the alarm server can be listed by executing

```
$ docker logs alarm-server
```

which should result in an output similar to the one listed below.

```
2017-07-07 20:31:12.560 INFO [Thread 1] org.csstudio.alarm.beast.server.Application (start) - Alarm Server 4.1.1.201701171407 started for 'LNLS_Alarms' configuration
Alarm Server 4.1.1.201701171407
Configuration Root: LNLS_Alarms
Database URL:       jdbc:postgresql://192.168.4.3:5432/lnls_alarms
JMS URL:            failover:(tcp://192.168.4.4:61616)
JMS Server Topic:   LNLS_Alarms_SERVER
JMS Client Topic:   LNLS_Alarms_CLIENT
JMS Talk Topic:     LNLS_Alarms_TALK
JMS Global Topic:   GLOBAL_SERVER
EPICS Addr. List:   10.0.4.57
2017-07-07 20:31:12.573 CONFIG [Thread 1] org.csstudio.vtype.pv.internal.Activator (start) - PV prefix pva provided by pva in org.csstudio.vtype.pv
2017-07-07 20:31:12.580 CONFIG [Thread 1] org.csstudio.vtype.pv.internal.Activator (start) - PV prefix ca provided by ca in org.csstudio.vtype.pv
2017-07-07 20:31:12.580 CONFIG [Thread 1] org.csstudio.vtype.pv.internal.Activator (start) - PV prefix ca provided by epics in org.csstudio.vtype.pv
2017-07-07 20:31:12.581 CONFIG [Thread 1] org.csstudio.vtype.pv.internal.Activator (start) - PV prefix loc provided by loc in org.csstudio.vtype.pv
2017-07-07 20:31:12.582 CONFIG [Thread 1] org.csstudio.vtype.pv.internal.Activator (start) - PV prefix sim provided by sim in org.csstudio.vtype.pv
2017-07-07 20:31:12.583 CONFIG [Thread 1] org.csstudio.vtype.pv.internal.Activator (start) - Default PV type ca
Read 0 PVs in 0.03 seconds: 0.0 PVs/sec
2017-07-07 20:31:13.376 INFO [Thread 39] org.apache.activemq.transport.failover.FailoverTransport (doReconnect) - Successfully connected to tcp://192.168.4.4:61616
```

Make sure that the following line are present in the output, because they aree an indicative that the connection with the other two containers succeeded. If they do not appear, it is necessary to check the file `docker-compose.yml` and verify if all IP addresses correspond to the ones specified in `configuration/LNLS-CON.ini`.

```
Read 0 PVs in 0.03 seconds: 0.0 PVs/sec
2017-07-07 20:31:13.376 INFO [Thread 39] org.apache.activemq.transport.failover.FailoverTransport (doReconnect) - Successfully connected to tcp://192.168.4.4:61616
```

<br>

##### Kubernetes 

<br>

###### Initial settings 

A good guide for the initial configuration of Kubernetes can be found through this [tutorial](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/getting_started_with_kubernetes), written by Red Hat.

<br>

###### Implementation 

In the folder `kubernetes` of the project [docker-alarm-composed](https://github.com/lnls-sirius/docker-alarm-composed), it's made available the configuration files of the ''pods'' and ''services'' required to run the alarm server, relational database and JMS module. The file names follow the pattern `beast-alarm-<component>-<type>.yaml`, in which `<component>` must be replaced by `server`, `postgres-db` or `activemq`  and `<type>`,  by `pod` or `service`.

In the files `beast-alarm-<component>-pod.yaml`, the environment variables, persistent volumes (only for the database) and exposed ports are defined. We highlight the attibute `containerPort`, which must be set according to the configuration file `configuration/LNLS-CON.ini`. 

In the files `beast-alarm-<component>-service.yaml`, in turn, we configure the ports through which the services are accessed:

* `clusterIP` is the service's IP address to which the other services can be connected.
* `port` represents an exposed port, which can receive or send requests from/to pods of other services.
* `nodePort` is exposed port in the '''host''', which can receive or send requests from/to external addresses.
* `targetPort`: requests sent to `port` or `nodePort` are redirected to `targetPort`. Therefore, this attribute must be equal to the parameter `containerPort` defined in `beast-alarm-<component>-pod.yaml`.

The script `kubernetes/kubectl-beast.sh`  submits the ''pods'' and ''services'' to Kubernetes. Make sure that `kubelet.service` is up, as explained in [[CON:Control-beast#Initial_settings|Initial settings]], and then submit the pods with command

```
$ kubernetes/kubectl-beast.sh add pod
```

and next submit the services by executing

```
$ kubernetes/kubectl-beast.sh add service
```

To remove them, run

```
$ kubernetes/kubectl-beast.sh del pod
$ kubernetes/kubectl-beast.sh del service
```

<br>

###### Debugging 

Debugging can be done in the same way than in [CON:Control-beast#Debugging|Debugging](link), but the command which accesses the logs of a container is

```
$ kubectl logs beast-alarm-server-pod
```

Before executing the above command, make sure that all pods and services are running. This can ba acomplished through the following commands.

```
$ kubectl get pods
$ kubectl get services
```

For the pods, all should be with their `STATUS` code equal to `Running`.

<br>

##### Docker Swarm 

<br>

###### Initial settings 

The initial configuration of this solution is simpler than the one of Kubernetes. Just follow the steps in [this tutorial](https://docs.docker.com/engine/swarm/swarm-tutorial/).

<br>

###### Implementation 

The implementation for this case is very similar with the solution provided for Docker Compose. Both use a file with extension `.yml`. A fundamental difference between them consists in the fact that [it is not possible to assign static addresses](https://github.com/moby/moby/issues/24170) to the services specified for a Swarm. Therefore, a container is identificated only with its hostname.

The use of this kind of solution must be associated with a distributed storage mechanism, such as the GlusterFS, since that the container execution will be distributed as well.

The deployment can be done by executing

```
$ docker stack deploy -c docker-swarm.yml beast-con
```

and its removal with

```
$ docker stack rm beast-con
```

<br>

###### Debugging 

The state of all services being executed by the swarm engine can be obtained with 

```
$ docker service ls
```

To list more details of a specific service, use the following command the service ''id'' given by the command above.

```
$ docker service ps <ID>
```

The logs of each service can be viewed with the command 

```
$ docker service logs <ID>
```

<br>

## *LNLS-Studio* as an alarm client 

In order to configure ''LNLS-Studio'' as an alarm server client, it's necessary to edit its preferences with the parameters of the PostgreSQL database server and ''JMS'' module. This can be done by editing the fields of `Window > Preferences > CSS Applications > Alarm > Alarm System`. Change `url`, `username` and `password`, as described previously, and restart ''LNLS-Studio''. If all was set correctly, it should be possible to visualize the configured alarms by opening three different views: ''Alarm tree'', ''Alarm table'' and ''Alarm area panel''. Such components are accessible from `Window > Show View > Other > CSS` or `Window > Open Perspective > Other > Alarm`.

The following figure exemplifies the use of ''LNLS-Studio':

<br>

|![](/img/machine/rf_system/CESR_cavity_SRF_module.png)|
|-|
|**Figure 2**: Use of *LNLS-Studio*|